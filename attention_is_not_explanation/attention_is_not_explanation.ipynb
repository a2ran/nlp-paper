{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11240f07",
   "metadata": {},
   "source": [
    "Attention is not Explanation paper review\n",
    "\n",
    "- DSL 논문 스터디 6기 손예진님이 발제하신 내용을 기반으로 작성했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb150f",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "<img src = '1.png'>\n",
    "\n",
    "The **Attention** model works by a specific attention mechanism ***\"Scaled Dot Product Attention\"*** <br>\n",
    "<br>\n",
    "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt(d_k)})$ <br>\n",
    "<br>\n",
    "\n",
    "Attention mechanism splits the input into three categories: <br>\n",
    "\n",
    "> ($Q$) for *query* (hidden-state at t decoder cell)<br>\n",
    "> ($K$) for *key*  (hidden-state at every encoder cell $k$)<br>\n",
    "> ($V$) for *value*  (hidden-state at every encoder cell $v$)\n",
    "\n",
    "<br>\n",
    "\n",
    "By calculating the **Dot product** of query $Q$ and key $K$, and scaling it by the **square root** of key $k$'s dimension, <br>\n",
    "The attention mechanism produce a **score** for each key value pair that indicates the importance of value $v$.\n",
    "\n",
    "<br>\n",
    "\n",
    "By computing scores for each value $v$, the **Attention** model is able to decide a particular important part of the **src sentences** it should attend to when making every predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661db2b",
   "metadata": {},
   "source": [
    "## Is Attention Explainable?\n",
    "\n",
    "Attention is not Explanation (Jain and Wallace; 2019, NAACL)\n",
    "\n",
    "<img src = '2.png'>\n",
    "\n",
    "The **Attention model** has been known to have ***Explainability***<br>\n",
    "since it can sort out **important input tokens** that affect the result of classification.\n",
    "\n",
    "For instance, by highlighting important tokens in the source sentences,<br> one can find out if the model is designed as it was meant to be, <br> and use it to **explain** the result of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39827ca",
   "metadata": {},
   "source": [
    "<img src = '3.png'>\n",
    "\n",
    "However, there are several reported cases of attention model **not properly** repersenting the explainability of tokens.\n",
    "\n",
    "*Attention Calibration for Transformer in Neural Machine Translation (Lu et al., ACL; 2021)* found out that some outputs (e.x. \"Deaths\") were tokenized from negligible tokens such as **[EOS]**.\n",
    "  \n",
    "Also, *What does BERT look at? An Analysis of BERT's Attention (Clark et al., BlackboxNLP; 2019* claims that most of attention results are tokenized from tokens such as [CLS], [SEP], Periods, and Commas.\n",
    "\n",
    "So, there are several **counterexamples** to use Attention to explain models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c4c22",
   "metadata": {},
   "source": [
    "## Paper Review\n",
    "\n",
    "***Main Opinion***: Attention is not a **faithful explanation** to a model.\n",
    "\n",
    "Attention model should satisfy following constraints to become explainable.\n",
    "\n",
    "> 1. Attention weights should be similar to other **explainable methods** such as **Feature Importance.** <br>\n",
    "> 2. There should be only one attention for a single output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04637b8",
   "metadata": {},
   "source": [
    "### Experiment 1-1\n",
    "\n",
    "<img src = '4.png'>\n",
    "\n",
    "**Experiment 1-1** explains correlation between **Attention Weights** and **Feature Importance** (Gradient/Leave One Out)\n",
    "\n",
    "**Gradient** calculates how change in a **particular token** affects the predicted value.\n",
    "\n",
    "$\\tau_g$ = correlation between Gradient and Attention.\n",
    "\n",
    "**Leave One Out (LOO)** calculates how excluding a **particular token** affects the overall performance of the model.\n",
    "\n",
    "$\\tau_{loo}$ = correlation between the output and attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab0df3",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "<img src = '5.png'>\n",
    "\n",
    "The **correlation coefficient** between Attention-Importance is lower than the **Average** value.\n",
    "\n",
    "$\\therefore$ Attention Weights have **low correlation** with Feature Importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c5e4e",
   "metadata": {},
   "source": [
    "### Experiment 1-2\n",
    "\n",
    "**Experiment 1-2** explores the correlations between feature importance and attention weights.\n",
    "\n",
    "<img src = '6.png'>\n",
    "\n",
    "### Result\n",
    "\n",
    "The correlation between LOO & Gradient is **significantly remarkable** than the correlation between Attention & Importance (0.2 ~ 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c0087",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "\n",
    "**Experiment 2** explores if it is possible to create another **attention** that yields same output with original attention model.\n",
    "\n",
    "<img src = '7.png'>\n",
    "\n",
    "**Total Variance Distance (TVD)** calculates difference between outputs\n",
    "\n",
    "**Jensen Shannon Divergence (JSD)** calculates difference between two distributions.\n",
    "\n",
    "**Counterfactual Attention** is an attention model that yields same output with original attention model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24daac48",
   "metadata": {},
   "source": [
    "### Experiment 2-1\n",
    "\n",
    "**Permutated Attention** randomly permutates attention to calculate a **total variance distance** between original attention output and the permutated attention output.\n",
    "\n",
    "**Experiment 2-1** searches if the counterfactual attention can be built from an original model with a miniscule difference in output.\n",
    "\n",
    "<img src = '8.png'>\n",
    "\n",
    "### Result\n",
    "\n",
    "<img src = '9.png'>\n",
    "\n",
    "Above graphs are the median difference between the $y$ from the original attention and $y$ from permutation attention.\n",
    "\n",
    "It is shown that there is no difference between permutated output and the original output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1dd6ca",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Experiment 1 :** Attention Weights is not related to Feature Importance. <br>\n",
    "However the correlation between Feature Importance is significant.\n",
    "\n",
    "**Experiment 2 :** There can be multiple attention for a single output. <br>\n",
    "The difference between Adversarial attention with original attention is miniscule.\n",
    "\n",
    "**As a Result**, Attention is not Explaination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ddf7e",
   "metadata": {},
   "source": [
    "출처 : https://arxiv.org/pdf/1902.10186.pdf<br>\n",
    "DSBA seminar material : http://dsba.korea.ac.kr/seminar/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
