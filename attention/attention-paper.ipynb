{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa189da",
   "metadata": {},
   "source": [
    "Seq2Seq paper review\n",
    "\n",
    "- DSL 논문 스터디 6기 손예진님이 발제하신 내용을 기반으로 작성했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a6691",
   "metadata": {},
   "source": [
    "## <span style = \"color : blue\"> Introduction </span>\n",
    "\n",
    "As mentioned prior, **Seq2seq** neural network model suffers huge information loss since it ***summerizes*** all its information into one single **context vector**. <br>\n",
    "Especially, if a train sequence is *lengthy*, the **Seq2seq** model was not able to translate it fluently.\n",
    "<br>\n",
    "***Attention*** model solves this problem by introducing **alignment** with **translation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ceecf",
   "metadata": {},
   "source": [
    "### <span style = \"color : skyblue\"> Model Architecture </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478fe4f",
   "metadata": {},
   "source": [
    "<img src = 'attention_1.png' width = '600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1aa1b",
   "metadata": {},
   "source": [
    "Instead of **Seq2seq** model summerizing all information from its ***Src sentences*** into one single **context vector**, <br>\n",
    "**Attention** model *\"attends\"* to a particular part of src sentences when making every predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d3f5a7",
   "metadata": {},
   "source": [
    "## <span style = \"color : blue\"> Paper Review </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e1532",
   "metadata": {},
   "source": [
    "<img src = 'attention_2.png' width = '300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8196fa0",
   "metadata": {},
   "source": [
    "The **Attention** model works by a specific attention mechanism ***\"Scaled Dot Product Attention\"*** <br>\n",
    "<br>\n",
    "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt(d_k)})$ <br>\n",
    "<br>\n",
    "\n",
    "Attention mechanism splits the input into three categories: <br>\n",
    "\n",
    "> ($Q$) for *query* (hidden-state at t-1 decoder cell)<br>\n",
    "> ($K$) for *key*  (hidden-state at every encoder cell)<br>\n",
    "> ($V$) for *value* \n",
    "\n",
    "<br>\n",
    "\n",
    "By calculating the **Dot product** of query $Q$ and key $K$, and scaling it by the **square root** of key $k$'s dimension, <br>\n",
    "The attention mechanism produce a **score** for each key value pair that indicates the importance of value $v$.\n",
    "\n",
    "<br>\n",
    "\n",
    "By computing scores for each value $v$, the **Attention** model is able to decide a particular important part of the **src sentences** it should attend to when making every predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6c384",
   "metadata": {},
   "source": [
    "### <span style = \"color : skyblue\"> Prediction Procedures </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55731d7",
   "metadata": {},
   "source": [
    "1. Get Attention Score for every decoder hidden state $s_{t-1}$ and encoder hidden state $h_i$\n",
    "\n",
    "<img src = 'attention_3.png' width = '600'>\n",
    "\n",
    "<br>\n",
    "\n",
    "Consider a depth-4 attention translation model.\n",
    "\n",
    "When predicting for $s_t$, first get an **Attention Score** from decoder hidden state $s_{t-1}$ and encoder hidden state $h_i$.\n",
    "\n",
    "$score(s_{t-1}, h_i) = e^t = {W_a}^Ttanh(W_{b}s_{t-1} + W_{c}h_{i})$\n",
    "\n",
    "where $W_a$, $W_b$, $W_c$ is a **trainable weight vectors**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b824ef1a",
   "metadata": {},
   "source": [
    "2. Use a softmax function to compute Attention Distribution\n",
    "\n",
    "By applying a **softmax function** to attention score $e^t$, get a probability distribution that sums up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547edd26",
   "metadata": {},
   "source": [
    "3. Sum up every Attention Weight and hidden-state to compute **Attention Value** (context vector)\n",
    "\n",
    "4. Compute $s_t$ from **Attention Value**\n",
    "\n",
    "5. Predict next value $v$ from the hidden state $s_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b5569",
   "metadata": {},
   "source": [
    "### <span style = \"color : skyblue\"> Results </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c9c7ec",
   "metadata": {},
   "source": [
    "<img src = 'attention_4.png' width = '800'>\n",
    "\n",
    "<img src = 'attention_5.png' width = '500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467d10c",
   "metadata": {},
   "source": [
    "1. **Attention model** (seqseq + attention) was able to surpass BLEU score of **Seq2seq model** in every instances.\n",
    "\n",
    "2. **Attention model** was able to surpass **Moses(= Traditional SMT)** despite trained from limited vocab sources.\n",
    "\n",
    "3. Since **Attention model** was trained with a ***\"soft alignment\"***, it is able to translate more fluently by relating to more similar vocabs.\n",
    "\n",
    "4. By making **context vector** from the particular part of **src sentences** the **Attention model** was able to solve information loss from summerizing all information in a **seq2seq model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6d7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
